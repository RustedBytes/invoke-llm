[package]
name = "invoke-llm"
version = "0.2.2"
edition = "2024"
description = "A command-line tool for querying OpenAI-compatible endpoints"
authors = [
  "Yehor Smoliakov <egorsmkv@gmail.com>",
]
keywords = ["llm", "nlp", "openai", "gemini", "hf-inference"]
categories = ["command-line-utilities"]
repository = "https://github.com/RustedBytes/invoke-llm"
license = "MIT"
readme = "README.md"

[dependencies]
clap = { version = "4.5.43", features = ["derive"] }
reqwest = { version = "0.12.22", features = ["json"] }
serde = { version = "1.0.219", features = ["derive"] }
serde_json = "1.0.142"
tokio = { version = "1.47.1", features = ["full"] }
anyhow = "1.0.98"
log = "0.4.27"
tracing = "0.1.41"
tracing-subscriber = "0.3.19"

[dev-dependencies]
tempfile = "3.20.0"

[profile.dev]
debug = false

[profile.release]
debug = false
strip = true
