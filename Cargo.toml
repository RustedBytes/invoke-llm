[package]
name = "invoke-llm"
version = "0.3.3"
edition = "2024"
description = "A command-line tool for querying OpenAI-compatible endpoints"
authors = [
  "Yehor Smoliakov <egorsmkv@gmail.com>",
]
keywords = ["llm", "nlp", "openai", "gemini", "hf-inference"]
categories = ["command-line-utilities"]
repository = "https://github.com/RustedBytes/invoke-llm"
license = "MIT"
readme = "README.md"

[dependencies]
clap = { version = "4.5.54", features = ["derive"] }
reqwest = { version = "0.12.26", features = ["json"] }
serde = { version = "1.0.228", features = ["derive"] }
serde_json = "1.0.145"
sentry = "0.46.0"
sentry-anyhow = "0.46.0"
tokio = { version = "1.48.0", features = ["full"] }
anyhow = "1.0.100"
tracing = "0.1.44"
tracing-subscriber = "0.3.22"

[dev-dependencies]
tempfile = "3.23.0"

[profile.dev]
debug = false

[profile.release]
debug = false
strip = true
