[package]
name = "invoke-llm"
version = "0.2.4"
edition = "2024"
description = "A command-line tool for querying OpenAI-compatible endpoints"
authors = [
  "Yehor Smoliakov <egorsmkv@gmail.com>",
]
keywords = ["llm", "nlp", "openai", "gemini", "hf-inference"]
categories = ["command-line-utilities"]
repository = "https://github.com/RustedBytes/invoke-llm"
license = "MIT"
readme = "README.md"

[dependencies]
clap = { version = "4.5.52", features = ["derive"] }
reqwest = { version = "0.12.24", features = ["json"] }
serde = { version = "1.0.228", features = ["derive"] }
serde_json = "1.0.145"
sentry = "0.36.0"
sentry-anyhow = "0.36.0"
tokio = { version = "1.48.0", features = ["full"] }
anyhow = "1.0.100"
tracing = "0.1.41"
tracing-subscriber = "0.3.20"

[dev-dependencies]
tempfile = "3.23.0"

[profile.dev]
debug = false

[profile.release]
debug = false
strip = true
